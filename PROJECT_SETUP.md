# AR Language Tutor - Complete Setup Guide

An **AR/VR language learning application** powered by AI for immersive conversational practice with virtual NPCs on Meta Quest 3.

[![Unity Version](https://img.shields.io/badge/Unity-6000.3.0f1-black.svg)](https://unity.com/)
[![Meta XR SDK](https://img.shields.io/badge/Meta%20XR%20SDK-v83.0.1-blue.svg)](https://developer.oculus.com/)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)

---

## ğŸ“‹ Table of Contents

1. [Overview](#overview)
2. [Prerequisites](#prerequisites)
3. [Installation](#installation)
4. [External Services Setup](#external-services-setup)
5. [Unity Project Configuration](#unity-project-configuration)
6. [Scene Setup](#scene-setup)
7. [VR/AR Setup (Meta Quest 3)](#vrar-setup-meta-quest-3)
8. [Testing & Troubleshooting](#testing--troubleshooting)
9. [Project Architecture](#project-architecture)
10. [Usage](#usage)

---

## ğŸ¯ Overview

This Unity project combines:
- **Augmented Reality (Meta Quest 3)** for immersive learning environments
- **Ollama + Llama3** for intelligent conversational AI (LLM)
- **Whisper.Unity** for accurate speech-to-text recognition
- **AllTalk TTS** for natural text-to-speech synthesis

The application provides an interactive language tutor NPC that can:
- âœ… Engage in natural conversations
- âœ… Correct grammar mistakes
- âœ… Teach vocabulary with context
- âœ… Provide pronunciation feedback
- âœ… Adapt to different learning scenarios

---

## ğŸ“¦ Prerequisites

### Required Software

1. **Unity Hub & Editor**
   - Version: `Unity 6000.0.3f1` (Unity 6)
   - [Download Unity Hub](https://unity.com/download)

2. **Git & Git LFS**
   - [Download Git](https://git-scm.com/downloads)
   - [Download Git LFS](https://git-lfs.com/)
   - Required for large asset files (models, audio)

3. **Ollama (LLama3 default, can be freely changed)**
   - [Download Ollama](https://ollama.ai/)
   - After installation, run: `ollama pull llama3`
   - Runs local LLM on port `11434`

4. **AllTalk TTS**
   - [AllTalk TTS (Beta Branch)](https://github.com/erew123/alltalk_tts/tree/alltalkbeta)
   - Follow their installation instructions
   - Runs on port `7851`

### Hardware Requirements

- **For Development**: Windows PC with 16GB+ RAM, GPU recommended
- **For Deployment**: Meta Quest 3 headset
- **Microphone**: Built-in Quest 3 mic or external

### Unity Packages (Auto-installed)

These packages are included in the project manifest:
- Meta XR SDK v83.0.1
- TextMeshPro
- Whisper.Unity (from GitHub)
- Input System

---

## ğŸš€ Installation

### Step 1: Install Git LFS

```bash
git lfs install
```

### Step 2: Clone the Repository

```bash
git clone https://github.com/yourusername/PAR_WS2526.git
cd PAR_WS2526
```

### Step 3: Pull LFS Assets

```bash
git lfs pull
```

### Step 4: Open in Unity

1. Open **Unity Hub**
2. Click **"Add"** â†’ Select the cloned `PAR_WS2526` folder
3. Unity version should match `6000.0.3f1` (Unity 6)
4. Click the project to open it
5. Wait for package imports and compilation (may take 5-10 minutes first time)

---

## âš™ï¸ External Services Setup

### 1. Start Ollama with Llama3

```bash
# Pull the model (first time only)
ollama pull llama3

# Start Ollama server
ollama serve
```

Verify it's running at: `http://localhost:11434`

### 2. Start AllTalk TTS

Follow [AllTalk TTS installation guide](https://github.com/erew123/alltalk_tts/tree/alltalkbeta):

```bash
# Navigate to AllTalk directory
cd alltalk_tts

# Activate environment (if using conda/venv)
conda activate alltalk  # or source venv/bin/activate

# Start the server
python server.py
```

Verify it's running at: `http://localhost:7851`

### 3. Verify Services

Test both services are accessible:

**Ollama:**
```bash
curl http://localhost:11434/api/generate -d '{"model":"llama3","prompt":"Hello"}'
```

**AllTalk:**
- Open browser to `http://localhost:7851`
- You should see the AllTalk web interface

---

## ğŸ® Unity Project Configuration

### Step 1: Create Configuration Assets

1. **In Unity Project window**, navigate to: `Assets/_Project/Scripts/Data/`

2. **Create ScriptableObjects** (Right-click in Project window):
   ```
   Create â†’ Language Tutor â†’ LLM Config â†’ Name: "DefaultLLMConfig"
   Create â†’ Language Tutor â†’ TTS Config â†’ Name: "DefaultTTSConfig"
   Create â†’ Language Tutor â†’ STT Config â†’ Name: "DefaultSTTConfig"
   Create â†’ Language Tutor â†’ Conversation Config â†’ Name: "DefaultConversationConfig"
   ```

### Step 2: Configure LLM Settings

**Select `DefaultLLMConfig`** and set:

```
LLM Service Configuration:
â”œâ”€ Service URL: http://127.0.0.1:11434
â”œâ”€ Endpoint Path: /api/generate
â”œâ”€ Model Name: llama3
â”œâ”€ Max Tokens: 512
â”œâ”€ Temperature: 0.7
â”œâ”€ Stream Response: âœ“ (checked)
â””â”€ Max Retries: 2

System Prompts:
â”œâ”€ Default Prompt: "You are a helpful language learning assistant..."
â”œâ”€ Grammar Correction: "You are a language tutor focused on grammar..."
â”œâ”€ Vocabulary Teaching: "You are a vocabulary tutor teaching new words..."
â””â”€ Conversation Practice: "You are a native speaker engaging in natural conversation..."
```

**Tip**: Customize prompts based on target language and learning goals.

### Step 3: Configure TTS Settings

**Select `DefaultTTSConfig`** and set:

```
TTS Service Configuration:
â”œâ”€ Service URL: http://127.0.0.1:7851
â”œâ”€ Endpoint Path: /api/tts-generate
â”œâ”€ Default Voice: male_01.wav  (or your preferred voice)
â”œâ”€ Default Language: en
â”œâ”€ Speech Rate: 1.0
â”œâ”€ Enable Caching: âœ“
â””â”€ Max Cache Size: 50
```

**Available voices**: Check AllTalk's `voices/` folder for options (male_01, female_01, etc.)

### Step 4: Configure STT Settings

**Select `DefaultSTTConfig`** and set:

```
STT Service Configuration:
â”œâ”€ Recording Settings:
â”‚  â”œâ”€ Max Recording Duration: 10 seconds
â”‚  â”œâ”€ Sample Rate: 16000
â”‚  â””â”€ Auto Stop On Silence: âœ“
â”‚
â””â”€ Whisper Settings:
   â”œâ”€ Model: base.en (or larger for better accuracy)
   â””â”€ Language: en
```

### Step 5: Configure Conversation Settings

**Select `DefaultConversationConfig`** and set:

```
Conversation Configuration:
â”œâ”€ Max History Length: 10 (last 10 exchanges)
â”œâ”€ Default Action Mode: Chat
â”œâ”€ Enable Context: âœ“
â”œâ”€ Show Subtitles: âœ“
â””â”€ Auto Play Response: âœ“
```

---

## ğŸ—ï¸ Scene Setup

### Open the Scene

1. Navigate to: `Assets/Scenes/`
2. Open `NPC_Test.unity` (or your main scene)

### Scene Setup (From Scratch)

If starting fresh:

#### 1. Add Meta Quest VR Camera Rig

1. **In Hierarchy**: Right-click â†’ **XR** â†’ **Meta Quest** â†’ **OVR Camera Rig**
2. Delete default Main Camera
3. Position OVRCameraRig at origin: `(0, 0, 0)`

#### 2. Add Whisper Manager

1. **In Hierarchy**: Right-click â†’ **Create Empty** â†’ Name: `WhisperManager`
2. **Add Component** â†’ Search: `Whisper Manager` (from Whisper.Unity package)
3. Configure:
   - Model: `base.en` or `small.en`
   - Language: `English`

#### 3. Add NPC Character

1. **In Project window**, navigate to:
   ```
   Assets/Assets/PartyPeople/Pack_FREE_PartyCharacters/Prefabs/
   ```
2. **Drag any character prefab** (e.g., `Character_0413da`) into scene
3. **Set Transform**:
   ```
   Position: X: 0, Y: 0, Z: 3  (3 meters in front of player)
   Rotation: X: 0, Y: 180, Z: 0  (facing camera)
   Scale: X: 1, Y: 1, Z: 1
   ```

#### 4. Add NPCController to Character

1. **Select your character** in Hierarchy
2. **Add Component** â†’ `NPCController` (or type `LanguageTutor.Core.NPCController`)
3. **Add Component** â†’ `NPCView` (handles UI and audio)
4. **Add Component** â†’ `Audio Source` (for TTS playback)
   - Volume: 1.0
   - Spatial Blend: 0 (2D for now)

---

## ğŸ¥½ VR/AR Setup (Meta Quest 3)

### Step 1: Create VR HUD Canvas

1. **In Hierarchy**: Right-click â†’ **UI** â†’ **Canvas**
2. Rename to: `VRHudCanvas`
3. **In Canvas Inspector**:
   ```
   Canvas:
   â”œâ”€ Render Mode: World Space (change from Screen Space)
   â””â”€ Event Camera: [Will assign in next step]
   ```

### Step 2: Parent Canvas to Camera

4. **Expand OVRCameraRig** in Hierarchy:
   ```
   OVRCameraRig
   â””â”€ TrackingSpace
      â””â”€ CenterEyeAnchor  â† This is the camera
   ```

5. **Drag `VRHudCanvas`** onto **CenterEyeAnchor** (make it a child)

6. **Set VRHudCanvas Transform** (local coordinates):
   ```
   Position: X: 0, Y: -0.2, Z: 1.5  (1.5m in front, slightly below eye level)
   Rotation: X: 0, Y: 0, Z: 0
   Scale: X: 0.001, Y: 0.001, Z: 0.001  (scales down to readable size)
   
   Rect Transform:
   â”œâ”€ Width: 800
   â””â”€ Height: 400
   ```

7. **Assign Event Camera**:
   - In Canvas component, drag **CenterEyeAnchor** to Event Camera field

### Step 3: Build HUD UI Elements

#### A) Background Panel (Optional)

8. **Right-click VRHudCanvas** â†’ **UI** â†’ **Image**
   - Rename: `HudBackground`
   - **Rect Transform**: Anchor preset â†’ Hold ALT+SHIFT â†’ Click "Stretch All"
   - Margins: 10 on all sides
   - **Image**: Color â†’ RGBA(0, 0, 0, 100) - dark semi-transparent
   - **Uncheck** "Raycast Target"

#### B) Subtitle Text

9. **Right-click VRHudCanvas** â†’ **UI** â†’ **Text - TextMeshPro**
   - (Import TMP Essentials if prompted)
   - Rename: `SubtitleText`
   - **Rect Transform**:
     ```
     Anchor: Top Center
     Position: X: 0, Y: -60
     Width: 700, Height: 120
     ```
   - **TextMeshPro Settings**:
     ```
     Text: "Subtitle text appears here..."
     Font Size: 28
     Alignment: Center (horizontal & vertical)
     Color: White
     Word Wrapping: âœ“
     Overflow: Ellipsis
     ```

#### C) Talk Button

10. **Right-click VRHudCanvas** â†’ **UI** â†’ **Button - TextMeshPro**
    - Rename: `TalkButton`
    - **Rect Transform**:
      ```
      Anchor: Bottom Center
      Position: X: 0, Y: 40
      Width: 200, Height: 70
      ```
    - **Button Image**: Color â†’ Blue (0, 120, 215)
    - **Expand TalkButton** â†’ Select child "Text (TMP)"
      - Text: "ğŸ¤ TALK"
      - Font Size: 32
      - Color: White
      - Bold: âœ“

#### D) Status Indicator

11. **Right-click VRHudCanvas** â†’ **UI** â†’ **Image**
    - Rename: `StatusIndicator`
    - **Rect Transform**:
      ```
      Anchor: Top Left
      Position: X: 40, Y: -40
      Width: 60, Height: 60
      ```
    - **Image**:
      - Color: Green (ready state)
      - Sprite: UI/UISprite
      - Preserve Aspect: âœ“

#### E) Mode Text

12. **Right-click VRHudCanvas** â†’ **UI** â†’ **Text - TextMeshPro**
    - Rename: `ModeText`
    - **Rect Transform**:
      ```
      Anchor: Top Right
      Position: X: -120, Y: -40
      Width: 200, Height: 50
      ```
    - **TextMeshPro**:
      - Text: "Chat Mode"
      - Font Size: 24
      - Alignment: Middle Right
      - Color: Cyan

### Step 4: Connect UI to NPCController

13. **Select your NPC character** in Hierarchy

14. **In NPCView component**, assign:
    ```
    UI Components:
    â”œâ”€ Subtitle Text â†’ Drag SubtitleText
    â”œâ”€ Talk Button â†’ Drag TalkButton
    â””â”€ Status Indicator â†’ Drag StatusIndicator
    
    Audio:
    â””â”€ Audio Source â†’ Should auto-fill
    ```

15. **In NPCController component**, assign:
    ```
    Configuration:
    â”œâ”€ Default LLM Config â†’ Drag DefaultLLMConfig
    â”œâ”€ Default TTS Config â†’ Drag DefaultTTSConfig
    â”œâ”€ Default STT Config â†’ Drag DefaultSTTConfig
    â””â”€ Default Conversation Config â†’ Drag DefaultConversationConfig
    
    Components:
    â”œâ”€ Npc View â†’ Drag NPCView (same GameObject)
    â””â”€ Whisper Manager â†’ Drag WhisperManager from Hierarchy
    ```

### Step 5: Enable Controller Interaction

#### Option A: Add OVR Input Module (Recommended)

16. **In Hierarchy**, select **EventSystem**
17. **In Inspector**:
    - Remove "Standalone Input Module" if present
    - **Add Component** â†’ `OVRInputModule`

#### Option B: Add Direct Button Input (Alternative)

18. **Select your NPC character**
19. **Add Component** â†’ **New Script** â†’ Name: `VRButtonInput`
20. **Copy this code**:

```csharp
using UnityEngine;
using UnityEngine.UI;

public class VRButtonInput : MonoBehaviour
{
    [SerializeField] private Button talkButton;

    void Update()
    {
        // A button on right controller triggers recording
        if (OVRInput.GetDown(OVRInput.Button.One))
        {
            talkButton?.onClick.Invoke();
        }
    }
}
```

21. **In Inspector**, drag **TalkButton** to the script's Talk Button field

### Step 6: Add Audio Listener (Critical!)

22. **Select CenterEyeAnchor** in Hierarchy
23. **Check if it has "Audio Listener" component**
    - If missing: **Add Component** â†’ **Audio Listener**
    - This is required to hear audio from the NPC's AudioSource!

---

## ğŸ§ª Testing & Troubleshooting

### Pre-Flight Checklist

Before pressing Play:

- âœ… Ollama running on port 11434
- âœ… AllTalk TTS running on port 7851
- âœ… All Config ScriptableObjects created and assigned
- âœ… WhisperManager in scene
- âœ… NPCController properly configured
- âœ… VRHudCanvas parented to CenterEyeAnchor
- âœ… Audio Listener on CenterEyeAnchor
- âœ… Microphone permission granted in Unity (Edit â†’ Project Settings â†’ Player â†’ Microphone Usage Description)

### Testing in Unity Editor

1. **Press Play** in Unity Editor
2. **Click the "ğŸ¤ TALK" button** with mouse (simulates VR controller)
3. **Speak into your microphone**
4. **Click button again** to stop recording
5. **Observe**:
   - Status indicator turns red (recording)
   - Subtitle shows your transcribed speech
   - Subtitle shows NPC response
   - NPC speaks (audio plays)

### Testing on Quest 3

#### Build Settings

1. **File** â†’ **Build Settings**
2. **Switch Platform** to **Android**
3. **Add Open Scenes** (your current scene)
4. **Player Settings**:
   ```
   Company Name: [Your Name]
   Product Name: AR Language Tutor
   
   XR Plug-in Management:
   â”œâ”€ Oculus: âœ“
   â””â”€ Initialize XR on Startup: âœ“
   
   Android Settings:
   â”œâ”€ Minimum API Level: 29
   â”œâ”€ Target API Level: 32+
   â”œâ”€ Package Name: com.yourcompany.languagetutor
   â””â”€ Install Location: Auto
   ```

5. **Connect Quest 3** via USB (Developer Mode enabled)
6. **Build and Run** â†’ Select output folder

### Common Issues & Solutions

#### ğŸ”´ No Audio Playback

**Problem**: NPC text appears but no sound  
**Solution**: 
- Check Audio Listener exists on CenterEyeAnchor
- Verify AudioSource Volume = 1.0
- Check system volume / Quest volume

#### ğŸ”´ Button Doesn't Respond

**Problem**: Clicking/pointing at button does nothing  
**Solution**:
- Verify OVRInputModule on EventSystem
- Check Canvas Event Camera is assigned
- Try VRButtonInput script (A button method)

#### ğŸ”´ No Subtitles Appear

**Problem**: Recording works but no text shown  
**Solution**:
- Check SubtitleText is assigned in NPCView
- Verify Ollama/AllTalk services are running
- Check Unity Console for error messages

#### ğŸ”´ Whisper Error: Model Not Found

**Problem**: "Whisper model not loaded"  
**Solution**:
- Select WhisperManager â†’ Download model from Inspector
- Or manually place model in `StreamingAssets/Whisper/`

#### ğŸ”´ LLM Connection Failed

**Problem**: "Connection refused to localhost:11434"  
**Solution**:
```bash
# Restart Ollama
ollama serve

# Verify it's running
curl http://localhost:11434/api/generate -d '{"model":"llama3","prompt":"Hi"}'
```

#### ğŸ”´ TTS Error: 500 Internal Server Error

**Problem**: AllTalk returns error  
**Solution**:
- Check AllTalk server logs
- Verify voice file exists in AllTalk's `voices/` folder
- Try default voice: `male_01.wav`

#### ğŸ”´ VR HUD Not Visible

**Problem**: UI doesn't appear in headset  
**Solution**:
- Check Canvas Scale is 0.001 (not 1.0)
- Verify Canvas is child of CenterEyeAnchor
- Check Z position is positive (1.5, not -1.5)

---

## ğŸ›ï¸ Project Architecture

### Folder Structure

```
Assets/
â”œâ”€ _Project/
â”‚  â”œâ”€ Scripts/
â”‚  â”‚  â”œâ”€ Core/              (Main logic)
â”‚  â”‚  â”‚  â”œâ”€ NPCController.cs
â”‚  â”‚  â”‚  â”œâ”€ ConversationPipeline.cs
â”‚  â”‚  â”‚  â”œâ”€ AudioInputController.cs
â”‚  â”‚  â”‚  â””â”€ ConversationHistory.cs
â”‚  â”‚  â”‚
â”‚  â”‚  â”œâ”€ Services/          (External API wrappers)
â”‚  â”‚  â”‚  â”œâ”€ ILLMService.cs
â”‚  â”‚  â”‚  â”œâ”€ OllamaService.cs
â”‚  â”‚  â”‚  â”œâ”€ ITTSService.cs
â”‚  â”‚  â”‚  â”œâ”€ AllTalkService.cs
â”‚  â”‚  â”‚  â”œâ”€ ISTTService.cs
â”‚  â”‚  â”‚  â””â”€ WhisperService.cs
â”‚  â”‚  â”‚
â”‚  â”‚  â”œâ”€ Actions/           (LLM action modes)
â”‚  â”‚  â”‚  â”œâ”€ ILLMAction.cs
â”‚  â”‚  â”‚  â”œâ”€ ChatAction.cs
â”‚  â”‚  â”‚  â”œâ”€ GrammarCheckAction.cs
â”‚  â”‚  â”‚  â”œâ”€ VocabularyTeachAction.cs
â”‚  â”‚  â”‚  â””â”€ ConversationPracticeAction.cs
â”‚  â”‚  â”‚
â”‚  â”‚  â”œâ”€ Data/              (Configuration ScriptableObjects)
â”‚  â”‚  â”‚  â”œâ”€ LLMConfig.cs
â”‚  â”‚  â”‚  â”œâ”€ TTSConfig.cs
â”‚  â”‚  â”‚  â”œâ”€ STTConfig.cs
â”‚  â”‚  â”‚  â””â”€ ConversationConfig.cs
â”‚  â”‚  â”‚
â”‚  â”‚  â”œâ”€ UI/                (View components)
â”‚  â”‚  â”‚  â””â”€ NPCView.cs
â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€ Utilities/
â”‚  â”‚     â””â”€ WavUtility.cs
â”‚  â”‚
â”‚  â””â”€ Documentation/
â”‚     â”œâ”€ ARCHITECTURE.md
â”‚     â”œâ”€ SETUP_GUIDE.md
â”‚     â””â”€ ENHANCED_NPC_SETUP.md
â”‚
â”œâ”€ Scenes/
â”‚  â””â”€ NPC_Test.unity
â”‚
â”œâ”€ Assets/                  (3rd party assets)
â”‚  â””â”€ PartyPeople/         (Character prefabs)
â”‚
â””â”€ Settings/               (Project settings)
```

### Key Design Patterns

- **Service-Oriented Architecture**: LLM, TTS, STT as injectable services
- **Command Pattern**: LLM actions (Chat, Grammar, Vocab, Practice) as swappable commands
- **MVC Pattern**: NPCController (Controller), ConversationPipeline (Model), NPCView (View)
- **Dependency Injection**: Services injected via ScriptableObject configs
- **Observer Pattern**: Events for state changes (recording start/stop, response ready)

### Component Flow

```
User Input (Voice) 
    â†“
AudioInputController (Microphone)
    â†“
WhisperService (STT) â†’ Transcription
    â†“
ConversationHistory (Context)
    â†“
LLMAction (Chat/Grammar/Vocab/Practice)
    â†“
OllamaService (LLM) â†’ Response Text
    â†“
AllTalkService (TTS) â†’ Audio Clip
    â†“
NPCView (Display subtitle + Play audio)
```

---

## ğŸ’¡ Usage

### Basic Conversation

1. Look at the NPC in VR
2. Point controller at "ğŸ¤ TALK" button and pull trigger (or press A button)
3. Speak your question/sentence
4. Press button again to stop recording
5. Wait for NPC response (subtitle + voice)

### Switching Modes

To change from Chat to Grammar/Vocabulary/Practice mode:

1. **In Unity Editor**: Select NPC â†’ NPCController â†’ Action Mode dropdown
2. **At Runtime**: Call `npcController.SetActionMode(ActionMode.GrammarCheck)` via script

### Example Interactions

**Chat Mode:**
- User: "Tell me about Paris"
- NPC: "Paris is the capital of France, known for..."

**Grammar Mode:**
- User: "I goed to the store yesterday"
- NPC: "The correct form is 'I went to the store.' The verb 'go' is irregular..."

**Vocabulary Mode:**
- User: "What does 'serendipity' mean?"
- NPC: "Serendipity means finding something good without looking for it..."

**Practice Mode:**
- NPC: "Let's practice ordering at a restaurant. I'll be the waiter..."

---

## ğŸ“š Additional Resources

- [Unity XR Documentation](https://docs.unity3d.com/Manual/XR.html)
- [Meta Quest Developer Center](https://developer.oculus.com/)
- [Whisper.Unity GitHub](https://github.com/Macoron/whisper.unity)
- [Ollama Documentation](https://ollama.ai/docs)
- [AllTalk TTS GitHub](https://github.com/erew123/alltalk_tts)

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---
